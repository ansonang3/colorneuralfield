#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Apr 22 11:19:30 2018

Last modified: Fri Dec 14 2018

@author: Anna SONG

Please see the related article for a global description of our algorithms.

For further information on pytorch and its usage, please refer to:
    [A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito,
    Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, Automatic differentiation in pytorch,
    in NIPS-W, 2017]

main_pytorch.py depends on the following scripts:
    - settings.py
    - visualize.py
    - pytorch_argmax.py
    - initialization.py

main_pytorch.py can:
    - simulate the Color Neural Field (CNF) (in the forward step),
    - above all, regress the 11 parameters of CNF stacked inside q, towards existing data.
        The automatic differentiation part is especially based on a backward step
        (half of which has been implemented by hand).

TO LAUNCH THE REGRESSION put descend_gradient = True

About the regression procedure:
    - the general structure: we do not use the automatic gradient descent tools proposed by pytorch,
        however we use pytorch only for simulating the predicted comparison results (forward)
        and for obtaining the gradient at that value of q (backward).
        We feed the current value and the gradient to scipy.optimize.minimize() and use the L-BFGS-B optimisation method.
    - the loss is a sum of terms like:   dist(comp_pred(q) - comp)
        in other words, we use computations to predict which color comp_pred(q) produces
        the nearest activity to that generated by the test configuration, and
        minimize this prediction with the comp given by experiments.
    - we can optimize along any subgroup of parameters, by nulling 
        the gradients of those which we currently do not want to vary.
    - note that softmax and softargmax are involved in the computations, leading to some small errors.
    - the comparison of color sensations is done through the use of a "perceptual distance" which
        we assume here to be the L^infinity distance
    
    -The loss optionally includes various constraints, such as :
        - parameter positivity : loss += negative_loss(parameter)
        - parameter magnitude order : loss += negative_loss(param1 - param2) etc.
        - ...
        
NOTES:
    - the syntax of pytorch has slightly changed, and in particular Variable and Tensor
        have been fused together. But at the time of this script writing, they were two distinct types.
        Since this is not embarrassing for now, we leave the current version as it is. 
    - Here it is a 1D color space constructed on the s-chromaticity (c = s - col_diff).

"""

import torch
import torch.nn as nn
from torch.autograd import Variable, Function
#from torch.autograd import gradcheck
from torch import Tensor
dtype = torch.FloatTensor

from scipy.optimize import minimize
from scipy.signal import fftconvolve

import time


import numpy as np
import matplotlib.pyplot as plt

from settings import date
if date not in [2004,2008]:
    raise ValueError('This script works only for the 2004 and 2008 settings.')

from settings import R,LC,Nc,Nx,dx,dc,loc,dr,color_abs,space_abs
from settings import white, test_configs, comp_configs
from settings import create_images

from visualize import write, compare, show_maps

from pytorch_argmax import SoftArgmax, SoftMax

#%%
''' IMPORTANT CONFIGURATIONS '''
descend_gradient = 1 # choose False or True to launch gradient descent or not

show = False # False by default, do not show the intermediate activities

# the parameters w.r.t. which we differentiate and allow gradient descent
we_diff = 'fg' # 'fg' by default (good in practice), only differentiating on parameters of f and g
print('We will differentiate on parameters of f and g only, but you can change this.')

if we_diff == 'fg' : # tune in physical and color space
    do_we_grad = {'muf':True, 'nuf':True, 'alphaf':True, 'betaf':True,
                  'mug':True, 'nug':True, 'alphag':True, 'betag':True,
                  'muh':False,'sigmah':False,'gamma':False}
    
elif we_diff == 'g' : # tune in physical space
    do_we_grad = {'muf':False, 'nuf':False, 'alphaf':False, 'betaf':False,
                  'mug':True, 'nug':True, 'alphag':True, 'betag':True,
                  'muh':False,'sigmah':False,'gamma':False}

elif we_diff == 'f' : # tune in color space
    do_we_grad = {'muf':True, 'nuf':True, 'alphaf':True, 'betaf':True,
                  'mug':False, 'nug':False, 'alphag':False, 'betag':False,
                  'muh':False,'sigmah':False,'gamma':False}

elif we_diff == 'hF' : # tune only input parameters and excitability
    do_we_grad = {'muf':False, 'nuf':False, 'alphaf':False, 'betaf':False,
                  'mug':False, 'nug':False, 'alphag':False, 'betag':False,
                  'muh':True,'sigmah':True,'gamma':True}

elif we_diff == 'all' : # tune all 11 parameters
    do_we_grad = {'muf':True, 'nuf':True, 'alphaf':True, 'betaf':True,
                  'mug':True, 'nug':True, 'alphag':True, 'betag':True,
                  'muh':True,'sigmah':True,'gamma':True}

else:
    raise ValueError('Please use a valid argument for we_diff')


parameter_names = ['muf','nuf','alphaf','betaf','mug','nug','alphag','betag','muh','sigmah','gamma']
subset_indices = [do_we_grad[x]*i for i,x in enumerate(parameter_names) if do_we_grad[x]]


Ncol = len(color_abs) 
Nexp = len(test_configs)

#%%

''' we choose one configuration (out of at least 8) to illustrate the optimization'''
if date == 2004 :
    exp0 = 6
if date == 2008 :
    exp0 = 2
    
config1 = test_configs[exp0]
config2 = comp_configs[exp0]
config3 = config2.copy() # will be such that config3[0] = comp_preds[exp0], when given some value of q, we have the predicted comparison value

    
#%%
    
def create_diffs(C) :
    '''for a stack of input cortical images C of shape NN * (2*Nx+1) * (2*Nx+1)
    create_diffs() creates a bunch of images of the form  c - C(r)
    where c is a variable in color_abs.
    This term will be given to the gaussian h so that we obtain
    the LGN input H.
    C is here supposed to be a scalar image, not a 3 dim color image.'''
    Cdiff = color_abs[None,None,None,:] - C[:,:,:,None]
    return Variable(torch.from_numpy(Cdiff).type(dtype))


def plotmap(fmap,gmap) :
    ''' show four small plots of g and f '''
    plt.subplot(2,2,1)
    plt.colorbar(plt.imshow(gmap,cmap = 'RdBu',vmin = -max(abs(gmap.min()),abs(gmap.max())), vmax = max(abs(gmap.min()),abs(gmap.max()))), orientation='vertical') # ticks ne marche pas : ticks = np.linspace(-1,1,2*Nc+1,endpoint = True)

    plt.subplot(2,2,2)
    plt.colorbar(plt.imshow(fmap,cmap = 'RdBu',vmin = -max(abs(fmap.min()),abs(fmap.max())), vmax = max(abs(fmap.min()),abs(fmap.max()))), orientation='vertical') # ticks ne marche pas : ticks = np.linspace(-1,1,2*Nc+1,endpoint = True)
    
    plt.subplot(2,2,3)
    plt.plot(gmap[Nx])
    plt.subplot(2,2,4)
    plt.plot(fmap[round(4/3*Nc)])
    plt.show()
    


#%%
''' The main operators involved in the layers of the big operator
a \mapsto F(w * a + H) '''

def Gauss(abscisse,sigma,mu = 1):
    gauss = lambda x : mu*np.exp(-(x**2)/(2*sigma**2))
    G = gauss(abscisse)
    return G

ext_X,ext_Y = np.mgrid[-R:R:(2*Nx+1)*1j,-R:R:(2*Nx+1)*1j]

class Gaussian_Conv(Function) :
    '''2D gaussian convolution on physical space (prepares for function g)'''

    @staticmethod
    def forward(ctx,a,sigma) :
        ctx.save_for_backward(a,sigma) # ici a et sigma sont sous forme de Tenseurs
        a_n, s_n = a.cpu().detach().numpy(), sigma.cpu().detach().numpy()[0]
        G = Gauss(sigma = s_n, abscisse = space_abs).reshape((1,2*Nx+1,1,1))
        fu = fftconvolve(a_n,G,'same')
        fu = fftconvolve(fu.transpose((0,2,1,3)),G,'same').transpose((0,2,1,3))
        return torch.from_numpy( fu ).type(dtype)
    
    @staticmethod
    def backward(ctx,grad_output): # ici a et sigma sont sous forme de Variables
        a,sigma = ctx.saved_tensors

        a_n =     a.data.cpu().numpy()
        s_n = sigma.data.cpu().numpy()[0]
        grad_output_n = grad_output.data.cpu().numpy()

        G = Gauss(sigma = s_n, abscisse = space_abs).reshape((1,2*Nx+1,1,1))
        fu = fftconvolve(grad_output_n,G,'same')
        fu = fftconvolve(fu.transpose((0,2,1,3)),G,'same').transpose((0,2,1,3))
        grad_a = Variable(torch.from_numpy( fu ).type(dtype))

        xy_filter = lambda x,y : (x**2+y**2)/(s_n**3)*np.exp(-(x**2+y**2)/(2*s_n**2))
        G = xy_filter(ext_X,ext_Y).reshape((1,2*Nx+1,2*Nx+1,1))
        aa = fftconvolve(a_n,G,'same')
        convo = Variable(torch.from_numpy(aa).type(dtype))
        grad_sigma = Tensor([torch.dot(convo.view(-1),grad_output.view(-1))])

        return grad_a,grad_sigma


def Flip_or_not(fw,flip) :
    ''' flips along the c coordinate in color space'''
    if not flip :
        return fw
    return np.ascontiguousarray(np.flip(fw,3)) # ascontiguousarray seems necessary for a good behavior in memory


class Gaussian_Conv_Color(Function) :
    '''1D convolution along color axis, with flip option (prepares for function f)'''
    
    @staticmethod
    def forward(ctx, a,sigma,flip) :
        ctx.save_for_backward(a,sigma)
        ctx.flip = flip
                             
        a_n, s_n = a.cpu().detach().numpy(), sigma.cpu().detach().numpy()[0]
        C = Gauss(sigma = s_n, abscisse = color_abs).reshape((1,1,1,2*Nc+1))
        fw= fftconvolve(a_n,C,'same')
        fw = Flip_or_not(fw,flip)
        return torch.from_numpy(fw).type(dtype)
    
    @staticmethod
    def backward(ctx,grad_output):
        a,sigma = ctx.saved_tensors
        flip = ctx.flip
        grad_flip = None

        a_n = a.data.cpu().numpy()
        s_n = sigma.data.cpu().numpy()
        grad_output_n = grad_output.data.cpu().numpy()

        C = Gauss(sigma = s_n, abscisse = color_abs).reshape((1,1,1,2*Nc+1))
        fw= fftconvolve(grad_output_n,C,'same')
        fw = Flip_or_not(fw,flip)
        grad_a = Variable(torch.from_numpy(fw).type(dtype))
        
        c_filter = lambda x : (x**2/s_n**3)*np.exp(-x**2/(2*s_n**2))
        C = c_filter(color_abs).reshape((1,1,1,2*Nc+1))
        fw = fftconvolve(a_n,C,'same')
        fw = Flip_or_not(fw,flip)
        convo = Variable(torch.from_numpy(fw).type(dtype))
        grad_sigma = Tensor([torch.dot(convo.view(-1),grad_output.view(-1))])
        
        return grad_a,grad_sigma,grad_flip


def Mexican_Hat(a,mug,nug,alphag,betag) :
    '''corresponding to function g'''
    GC = Gaussian_Conv().apply
    return mug * GC(a, alphag) - nug * GC(a, betag)

def SOG(a,muf,nuf,alphaf,betaf) :
    '''corresponding to function f (a non-symmetric `sum' of gaussians)'''
    GCC = Gaussian_Conv_Color().apply
    return muf * GCC(a,alphaf,False) - nuf * GCC(a,betaf,True)


class Sigmoid(nn.Module):
    '''corresponding to function F, a sigmoid function passing through 1/2 at x = 0 '''
    def __init__(self):
        super(Sigmoid,self).__init__()
        self.gamma = nn.Parameter(torch.Tensor([5.]).type(dtype),requires_grad = do_we_grad['gamma'])
    
    def forward(self,a):
        F = lambda x : 1./(1+torch.exp(-self.gamma*x))
        return F(a)

class ColorPeak(nn.Module):
    ''' corresponds to function H, the LGN input or `forcing term' '''
    def __init__(self):
        super(ColorPeak,self).__init__()
        self.muh = nn.Parameter(torch.Tensor([.2]).type(dtype),requires_grad = do_we_grad['muh'])
        self.sigmah = nn.Parameter(torch.Tensor([.5]).type(dtype),requires_grad = do_we_grad['sigmah'])
        
    def forward(self,Diff) :
        h = lambda x : self.muh*torch.exp(-(x**2)/(2*self.sigmah**2))
        H = h(Diff)
        return H

#%%
        
''' This is the core of the script '''

class Net(torch.nn.Module):
    def __init__(self):
        """
        This is the main Wilson Cowan algorithm, which assembles multiple
        operations on an arbritrary initial activity a(r,c). The final activity
        a(r_0,c), where r_0 is a point of interest, comes from a(r,c) stationary
        point of a = F(a,q) (found after 15-20 iterations)
        where q is the set of parameters to be minimized.
        """
        super(Net, self).__init__()
        
        self.f = SOG
        self.muf = nn.Parameter(torch.Tensor([1.0]).type(dtype),requires_grad = do_we_grad['muf'])
        self.nuf = nn.Parameter(torch.Tensor([1.3]).type(dtype),requires_grad = do_we_grad['nuf'])
        self.alphaf = nn.Parameter(torch.Tensor([0.8]).type(dtype),requires_grad = do_we_grad['alphaf'])
        self.betaf = nn.Parameter(torch.Tensor([0.9]).type(dtype),requires_grad = do_we_grad['betaf'])
        
        self.g = Mexican_Hat
        self.mug = nn.Parameter(torch.Tensor([3.0]).type(dtype),requires_grad = do_we_grad['mug'])
        self.nug = nn.Parameter(torch.Tensor([3.0]).type(dtype),requires_grad = do_we_grad['nug'])
        self.alphag = nn.Parameter(torch.Tensor([0.2]).type(dtype),requires_grad = do_we_grad['alphag'])
        self.betag = nn.Parameter(torch.Tensor([0.5]).type(dtype),requires_grad = do_we_grad['betag'])

        self.F = Sigmoid()
        self.H = ColorPeak()
        
        self.Argmax = SoftArgmax()
        self.Max = SoftMax()

    def forward(self,Diff,loc,N = Nexp):
        a = Variable(.5*torch.ones(N + Ncol,2*Nx+1,2*Nx+1,2*Nc+1))
        for _ in range(15) : # 15 to 20 iterations are sufficient for the activities to converge
            a = self.g(a,self.mug,self.nug,self.alphag,self.betag)
            a = self.f(a,self.muf,self.nuf,self.alphaf,self.betaf)
            a = dc*dx**2*a # do not forget for a real integration
            a = self.F(a + self.H(Diff))
        a = a[:,Nx,loc,:] # we restrict to the point of interest r_0, indicated by loc from settings.py
        a_tests = a[:N] # shape N * Ncol; for all the test patterns of the experiments
        as_compared = a[N:] # shape Ncol * Ncol; all the comparison patterns have to be tested (but the same family is used for all experiments)

        u = torch.abs(as_compared[None,:,:] - a_tests[:,None,:]) # shape (N = Nexp,Ncol,Ncol)
        u = self.Max(u) # shape (N = Nexp,Ncol), looking for L^infinity norm between (simulated) comps and tests sensations
        u = 1/(u+1) # important to look for min instead of max
        comp_preds = self.Argmax(u) # shape (Nexp,); looking for the comparison pattern MINIMIZING the L^infinity norm w.r.t. test sensation
        return comp_preds,a_tests,as_compared

    def from_numpy(self, q):
        ''' This feeds the model.parameters with a new value for q
        when it is a numpy array '''
        muf,nuf,alphaf,betaf,mug,nug,alphag,betag,muh,sigmah,gamma = q
        self.muf.data = torch.from_numpy(np.array([muf])).type(dtype)
        self.nuf.data = torch.from_numpy(np.array([nuf])).type(dtype)
        self.alphaf.data = torch.from_numpy(np.array([alphaf])).type(dtype)
        self.betaf.data = torch.from_numpy(np.array([betaf])).type(dtype)
        self.mug.data = torch.from_numpy(np.array([mug])).type(dtype)
        self.nug.data = torch.from_numpy(np.array([nug])).type(dtype)
        self.alphag.data = torch.from_numpy(np.array([alphag])).type(dtype)
        self.betag.data = torch.from_numpy(np.array([betag])).type(dtype)
        self.H.muh.data = torch.from_numpy(np.array([muh])).type(dtype)
        self.H.sigmah.data = torch.from_numpy(np.array([sigmah])).type(dtype)
        self.F.gamma.data = torch.from_numpy(np.array([gamma])).type(dtype)


model = Net()

#%%
def show_comp(q, save = False) :
    ''' Given a value for arbitrary parameters q , 
    show the test, comp and pred activities for all configurations.
    However it only displays the activity generated for experiment nÂ° exp0,
    thanks to compare() from visualize.py, which emulates the evolution of the CNF using numpy (not pytorch):
        - test + config1_test (blue)
        - comp + config1_comp (orange)
        - comp_predicted + config1_comp (green)
    If q is a local min, the curves for comp and comp_pred must be as close as possible,
    and not too far from that of test.
    '''

    model.from_numpy(q)
    test_and_other_configs = np.concatenate((test_configs,np.array([[compared, white, white] for compared in color_abs])))
    C = create_images(test_and_other_configs)
    Cdiff = create_diffs(C)
    comp_preds,_,_ = model(Cdiff,loc)
    comp_preds = comp_preds.data.numpy()
    
    config3[0] = comp_preds[exp0]
    DT = 0.1 # allows to be sure that we were not biased by the fixed point iteration
    # so we intentionnally test on an Euler scheme of time pace DT
    print('comparing test, predicted and comparison sensations with dt = {}'.format(DT))
    compare(q,config1,config2,config3 = config3,show = False,dt = DT, save = save)
    
    comps = comp_configs[:,0]
    # this figures compare the predictions and real data for the 8 different configurations (pp,ll,pw,wp, etc.)
    plt.figure()
    plt.scatter(range(Nexp),comps,s = 50, c ='r',marker = 'o')
    plt.scatter(range(Nexp),comp_preds,s = 70, c ='b',marker = 'x')
    plt.title('f %.2f, %.2f, %.2f, %.2f, g %.2f, %.2f, %.2f, %.2f, h %.2f, %.2f, F %.2f' % tuple(q))
    plt.show()
    
def negative_loss(p) :
    return torch.min(Variable(torch.zeros(1),requires_grad = False),p)**2

def subset_parameters(model) : # to know w.r.t. which variables we want to differentiate
    return [p for p in model.parameters() if p.requires_grad]

criterion = lambda c1,c2 : ((c1 - c2)**2).sum()
tnf = 0 # Total Number of Function evaluations, as in scipy.optimize.minimize

def Loss_and_grads(q) :
    ''' Loss_and_grads(q) returns two values: the current loss, and the gradients
    corresopnding to parameters wrt which we want to differentiate.
    It relies on the model = Net() created using pytorch.
    This wraps the input used to feed scipy.optimize.minimize
    '''
    print('\n')
    write(q)
    
    global tnf
    tnf += 1

    loss = 0
    
    model.from_numpy(q)
    # create the stack of images {c - C(r)} where c is in color_abs and C varies in the stack of cortical images
    test_and_other_configs = np.concatenate((test_configs,np.array([[compared, white, white] for compared in color_abs])))
    C = create_images(test_and_other_configs)
    Cdiff = create_diffs(C)
    comp_preds,a_tests,_ = model(Cdiff,loc)
    comps = comp_configs[:,0]    
    
    # what we want to optimize
    loss += 100*criterion(comp_preds,Variable(torch.from_numpy(comps).type(dtype)))
    print('tnf = %.0f  current loss: ' %tnf, loss.item())

    if tnf%4== 1 : # every four tnf iterations, illustrate the current parameter q 
        print('test, comp and comp_pred are ',test_configs[exp0,0],comp_configs[exp0,0],comp_preds[exp0].item())
        if __name__ == '__main__' :
            config3[0] = comp_preds[exp0]
            compare(q,config1,config2,config3 = config3,show = False)
            # this figures compare the predictions and real data for the 8 different configurations (pp,ll,pw,wp, etc.)
            plt.figure()
            plt.scatter(range(Nexp),comps,s = 15, c ='r',marker = 'o')
            plt.scatter(range(Nexp),comp_preds.data.numpy(),s = 15, c ='b',marker = 'x')
            plt.title('f %.2f, %.2f, %.2f, %.2f, g %.2f, %.2f, %.2f, %.2f, h %.2f, %.2f, F %.2f' % tuple(q))
            plt.show()
    
    # now it's time to differentiate
    parameters = subset_parameters(model)
    dq_loss = torch.autograd.grad(loss,parameters)
    dq_loss_numpy = np.zeros_like(q)
    for i,x in enumerate(subset_indices) :
        dq_loss_numpy[x] = dq_loss[i]
    print('dq_loss_numpy',dq_loss_numpy) # to check the gradient
    return loss.data.numpy().astype('float64'), dq_loss_numpy.astype('float64')


def grad_desc(q,show = False) :
    ''' Launches the gradient descent.
    In the end, displays the results of the minimization. '''
    res = minimize(lambda q : Loss_and_grads(q), # function to minimize
						q, # starting estimate
						method = 'L-BFGS-B',  # an order 2 method
						jac = True,  # use the gradient returned by main function Loss_and_grads(q)
						options = dict(maxiter = 60,ftol = 1e-3, gtol = 1e-3, maxcor = 10, disp = True))
#    print('final gradients')
#    for y in res.jac :
#        print(y)
    print('final values')
    for y,name in zip(res.x,parameter_names) :
        print(name,y)
    if show :
        show_maps(res.x)
#    for config1,config2 in zip(test_configs,comp_configs) :
#        compare(res.x,config1,config2,show = False) # this is the arbitrary configuration chosen to keep track of the final q_localmin.
    write(res.x,letter = False)
    show_comp(res.x)
    return res


#%%
if date == 2008:
    from settings import test_configs,comp_configs, verif_configs
    ''' Emulates the nonlinear matching shifts for [Monnier (2008)].
        In particular, we can see whether the q_min given by minimizing over [Monnier & Shevell (2004)]
        can reproduce the experimental results from [Monnier (2008)].'''
        
    def verify_nonlinear(q,mod) :
        print('Now it is time to test whether we can reproduce non linearity.')
        Lverif = len(verif_configs)
        test_and_other_configs = np.concatenate((verif_configs,np.array([[compared, white, white] for compared in color_abs])))
        C = create_images(test_and_other_configs, dr = dr)
        Cdiff = create_diffs(C)
        mod.from_numpy(q)
        comp_preds,a_tests,_ = mod(Cdiff,loc,len(verif_configs))
        pred_results = (comp_preds.data).numpy()
        
        plt.figure(figsize = (6,6))
        plt.plot(verif_configs[:Lverif//2,0],pred_results[:Lverif//2]-verif_configs[:Lverif//2,0])
        plt.plot(verif_configs[Lverif//2:,0],pred_results[Lverif//2:]-verif_configs[Lverif//2:,0])
        plt.scatter(test_configs[:,0],comp_configs[:,0] - test_configs[:,0],c = 'r',marker = 'x',s = 30)
        plt.title('f %.2f, %.2f, %.2f, %.2f, g %.2f, %.2f, %.2f, %.2f, h %.2f, %.2f, F %.2f' % tuple(q))
        plt.axis([verif_configs.min()-0.1,verif_configs.max()+0.1,-0.8-0.05,0.8+0.05])
        plt.show()
        return pred_results


#%%
if __name__ == '__main__': 
    # for the default values, Nx = 10, Nc = 20, do_we_grad = 'fg',
    # GD will typically take about 10 minutes so it is quite long: be patient!
    # also, the first iterations are often slower than the following ones
    from initialization import q_Bob, q_nonlin
    if date == 2004 :
        q = q_Bob
        print('initializing with q = {}'.format(np.round(q,2)))
    if date == 2008 :
        q = q_nonlin
        print('initializing with q_nonlin')
        pred_results = verify_nonlinear(q,model) # to test if this reproduces nonlinearity or not


    if descend_gradient :
        show_maps(q)
        t = time.clock()
        
        res = grad_desc(q,show = True)
        print(time.clock() - t, "seconds to make GD")
    
        if date == 2008: # you can also test on the final value of q optimized for date = 2004
            pred_results = verify_nonlinear(res.x,model)
